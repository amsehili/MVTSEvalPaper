{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The following is the code to produce figure 2 to 5 in the paper, with further explanations. For a definition of the **point-adjust** protocol and the way it computes scores, please see the corresponding section in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from scipy.stats import binom\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_matplotlib(figsize=(16, 6), bgcolor=\"#FFFFFF\"):\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = figsize\n",
    "    matplotlib.rcParams[\"figure.facecolor\"] = bgcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacking point-adjust by randomly tagging points as anomalous\n",
    "In the following, we consider four datasets that have different sizes but the same contamination rate of $10\\%$ (that is, the number of anomalous points to the total number of points). Moreover, anomalous points make up one single segment of contiguous points of size $A$ for each dataset.\n",
    "\n",
    "For each dataset, we compute the probability of having a perfect **point-adjust** recall by randomly drawing $\\alpha$ points from the dataset and tagging them as anomalous, where $\\alpha$ is $1\\%$ of the total number of points. In the following, we show that large values of $A$ yield larger F1 scores with higher confidence.\n",
    "\n",
    "Each subplot correspond to a dataset. The x-axis represents given F1 **point-adjust** values (referred to as $\\text{F1}_{pa}$) and the y-axis the probability of having an $\\text{F1}_{pa}$ value $\\le$ a given value on the x-axis.\n",
    "More precisely, this is the Cumulative Distribution Function (CDF) of $\\text{F1}_{pa}$ for each dataset. For convenience, the figure doesn't show the probability for $\\text{F1}_{pa}=0$, which is shown in the next figure for the 1st and 4th datasets.\n",
    "\n",
    "The main takeaway from this figure is that for the same algorithm (''algorithm'' here is a randomly tagging of $1\\%$ points as anomalous) large anomalous segments make the score look larger because selecting a point from within the segment does not have the same impact on the different datasets. Typically, we note that while **point-adjust** Recall, $\\text{R}_{pa}$ is 1 if at least one randomly selected point lies within the segment, **point-adjust** Precision, $\\text{P}_{pa}$ is higher as the difference for larger values of $A$. This is true because if one random point falls within the anomalous segment $\\text{P}_{pa} = \\frac{A}{A + (\\alpha - 1)}$ and we have $\\lim_{A\\to\\infty} \\left[\\frac{A}{A + (\\alpha - 1)}\\right] = 1$.\n",
    "\n",
    "Of course, the more random points falling within the anomalous segment, the better the precision. But here we are interested in calculating the value of **minimum** precision (and the minimum $\\text{F1}_{pa}$ score that corresponds to it) reached when just one randomly selected point falls within the anomalous segment and the probability that that happens. This is the probability of having *at least* one random point within the anomalous segment, computed as: $1 - the~probability~of~having~all~random~points~outside~the~segment = 1 - (1 - r)^{\\alpha}$. This probability is given by $p(\\text{R}_{pa}=1)$ in the title of each subplot.\n",
    "\n",
    "Note that strictly speaking, the probability of reaching a perfect **point-adjust** recall is $p(\\text{R}_{pa}=1) = \\sum_{n=1}^{\\alpha}{p(\\text{TP}=n)}$, where $\\text{TP}$ is the **point-wise** True Positive. This is the sum of probabilities of selecting one up to ${\\alpha}$ points from the anomalous segment, but in the figure, $p(\\text{R}_{pa}=1)$ corresponds to selecting exactly one point from the anomalous segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r: contamination rate\n",
    "# A: number of anomalous points\n",
    "# T: total number of points (T = A / r)\n",
    "# h: percentage of random points tagged as anomalous\n",
    "# alpha: number of random points tagged as anomalous (alpha = int(T * h))\n",
    "\n",
    "init_matplotlib(figsize=(16, 6))\n",
    "\n",
    "r = 0.1\n",
    "h = 0.01\n",
    "\n",
    "for i, A in enumerate([50, 100, 250, 500]):\n",
    "\n",
    "    T = A / 0.1\n",
    "    alpha = int(T * h)\n",
    "\n",
    "    # nb hits of anomalous segment\n",
    "    n_hits = np.arange(0, alpha + 1)\n",
    "    # point-adjust True Positive\n",
    "    TP_pa = np.array([0 if n == 0 else A for n in n_hits])\n",
    "\n",
    "    # probability of perfect point-adjust recall\n",
    "    proba_perfect_R_pa = 1 - (1 - r) ** alpha\n",
    "\n",
    "    # point-adjust precision for each number of hits\n",
    "    # this is an array, so we have one P_pa value for each n_hits\n",
    "    P_pa = TP_pa / (TP_pa + alpha - n_hits)\n",
    "\n",
    "    # point-adjust F1 score\n",
    "    # point-adjust Recall = 1 (with different probabilities!)\n",
    "    # for one single anomalous segment. This is also an array\n",
    "    F1_pa = (2 * P_pa * 1) / (P_pa + 1 + 1e-10)\n",
    "\n",
    "    # we can use the binomial distribution to compute the probability\n",
    "    # of hitting the anomalous segment a given number of times. We have:\n",
    "    # n=alpha, k=n_hits and p=r\n",
    "    \n",
    "    # We're more interested in the Cumulative Distribution Function for each number\n",
    "    # of hits, that is, the probabilitity of hitting the anomalous segments at least\n",
    "    # 'n_hits'. These are also the probabilitities of obtaining a point-adjust F1\n",
    "    # scores below the values computed in F1_pa\n",
    "    cdf = binom.cdf(k=n_hits, n=alpha, p=r)\n",
    "\n",
    "\n",
    "    # For convenience, we skip the first value in F1_pa and cdf\n",
    "    # See next figure with this value included\n",
    "    F1_pa, cdf = F1_pa[1:], cdf[1:]\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(F1_pa, cdf, \".-\")\n",
    "\n",
    "    plt.title(\n",
    "        f\"$A={A}, \\quad \\\\alpha={alpha}, \\quad$\"\n",
    "        + \"$p($\" + \"R$_\"\n",
    "        + \"{\"\n",
    "        + \"pa\"\n",
    "        + \"}=1)=\"\n",
    "        + f\"{round(proba_perfect_R_pa, 2)}$\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    indices = [0, len(F1_pa) // 4, len(F1_pa) // 2, int(len(F1_pa) / 4 * 3), len(F1_pa) - 1]\n",
    "    x_ticks_pos = F1_pa[indices]\n",
    "    x_ticks_val = np.round(x_ticks_pos, 3)\n",
    "    plt.xticks(x_ticks_pos, x_ticks_val)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the probability of having a null $\\text{F1}_{pa}$  score\n",
    "This figure is comparable to the previous one except that we include the probability of having $\\text{F1}_{pa}=0$, and we only consider the two datasets with $A=50$ and $A=500$. This illustrates how big the jump in score can be after hitting the anomalous segment one single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_matplotlib(figsize=(16, 3))\n",
    "\n",
    "r = 0.1\n",
    "h = 0.01\n",
    "\n",
    "for i, A in enumerate([50, 500]):\n",
    "\n",
    "    T = A / 0.1\n",
    "    alpha = int(T * h)\n",
    "\n",
    "    # nb hits of anomalous segment\n",
    "    n_hits = np.arange(0, alpha + 1)\n",
    "    # point-adjust True Positive\n",
    "    TP_pa = np.array([0 if n == 0 else A for n in n_hits])\n",
    "\n",
    "    # probability of perfect point-adjust recall\n",
    "    proba_perfect_R_pa = 1 - (1 - r) ** alpha\n",
    "\n",
    "    # point-adjust precision for each number of hits\n",
    "    P_pa = TP_pa / (TP_pa + alpha - n_hits)\n",
    "    F1_pa = (2 * P_pa * 1) / (P_pa + 1 + 1e-10)\n",
    "\n",
    "    cdf = binom.cdf(k=n_hits, n=alpha, p=r)\n",
    "\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.plot(F1_pa, cdf, \".-\")\n",
    "\n",
    "    plt.title(\n",
    "        f\"$A={A}, \\quad \\\\alpha={alpha}, \\quad$\"\n",
    "        + \"$p($\" + \"R$_\"\n",
    "        + \"{\"\n",
    "        + \"pa\"\n",
    "        + \"}=1)=\"\n",
    "        + f\"{round(proba_perfect_R_pa, 2)}$\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worst $\\text{F1}_{pa}$ and $\\text{P}_{pa}$ and associated probability\n",
    "In this figure, we consider a dataset with $A=50$ and the same contamination rate of $0.1$. For different values of $\\alpha$ (from $1$ to $30$), we compute the probability of randomly selecting one single point from the anomalous segment (leading to a perfect **point-adjust** recall) and the remaining $\\alpha - 1$ points from outside the segment, which corresponds to the worst possible values of F1$_{pa}$ and P$_{pa}$ values when R$_{pa}=1$. We see that certain values of $\\alpha$ yield a fairly high F1$_{pa}$ score for a random guess algorithm, with high probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_matplotlib(figsize=(8, 3), bgcolor=\"#FFFFFF\")\n",
    "A = 50\n",
    "alpha = np.arange(1, 31)\n",
    "proba_perfect_R_pa = 1 - (1 - r) ** alpha\n",
    "\n",
    "P = A / (A + alpha - 1)\n",
    "F1 = (2 * P * 1) / (P + 1)\n",
    "\n",
    "label = \"$p($\" + \"R$_\" + \"{\" + \"pa\" + \"}=1)~$\" + \"($\\\\alpha$)\"\n",
    "\n",
    "plt.xlabel(label, fontsize=14)\n",
    "# plt.ylabel(\"Precision\")\n",
    "\n",
    "x_ticks_pos = proba_perfect_R_pa\n",
    "x_ticks_val = np.array([f\"{round(p, 3)} ({a})\" for p, a in zip(proba_perfect_R_pa, alpha)])\n",
    "indices = [0, 4, 9, 14, -5]\n",
    "plt.xticks(x_ticks_pos[indices], x_ticks_val[indices], fontsize=11)\n",
    "\n",
    "plt.plot(proba_perfect_R_pa, F1, label=\"Worst F1$_{pa}$\")\n",
    "plt.plot(proba_perfect_R_pa, P, \"--\", label=\"Worst P$_{pa}$\")\n",
    "_ = plt.legend(fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise F1 score of a given detector with datasets of different contamination rates\n",
    "In this figure, we consider many anomaly detectors with the same probability ($0.99$) of classifying a point as anomalous when the point is actually anomalous, but with different False Alarm Rates (FAR). For each detector (one tick on the x-axis), we compute the expected **point-wise** F1 score with three different datasets. The datasets have different contamination rates: $0.33$, $0.091$ and $0.01$.\n",
    "\n",
    "We see that for almost all detectors, the F1 score is better for datasets with higher contamination rates. Particularly, we observe that the difference between the best and the worst detectors is the smallest for the dataset with the highest contamination rate.\n",
    "\n",
    "The goal of this part is to show that, for the same detector, the **point-wise** performance may vary greatly as a function of the contamination rate of the dataset. It is reminiscent of the main [weakness](https://datascience.stackexchange.com/questions/110124/what-are-the-disadvantages-of-accuracy) of the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for which the F1 score is the recommended alternative in the first place. Actually, in extreme cases where the contamination rate is very high, the F1 score becomes very high across all detectors (try adding a big number such as $20000$ to the list in line `zip([5000, 1000, 100] ...`).\n",
    "\n",
    "The FAR and Recall of a given detector, however, are constant across datasets and have a much more intuitive interpretation than the F1 score. This makes them very interesting for evaluating anomaly detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# n_normal: number of normal points in data\n",
    "# n_anomalous: number of anomalous points in data\n",
    "# proba_ano_ano: probability that the detector correctly detects an anomalous point as such\n",
    "# FAR: False Alarm Rate of detector\n",
    "\n",
    "init_matplotlib((9, 4), bgcolor=\"#FFFFFF\")\n",
    "\n",
    "n_normal = 10000\n",
    "proba_ano_ano = 0.99\n",
    "FAR = np.arange(0.001, 0.202, 0.01)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "for n_anomalous, fmt in zip([5000, 1000, 100], [\"-\", \"--\", \"-.\"]):\n",
    "    y = []\n",
    "    for far in FAR:\n",
    "        TP = proba_ano_ano * n_anomalous\n",
    "        FP = far * n_normal\n",
    "        FN = n_anomalous - TP\n",
    "        P = (TP) / (TP + FP)\n",
    "        R = TP / (TP + FN)\n",
    "        assert R == proba_ano_ano\n",
    "\n",
    "        F1 = (2 * P * R) / (P + R)\n",
    "        y.append(F1)\n",
    "\n",
    "    y = np.array(y)\n",
    "    ax.plot(FAR, y, fmt, label=f\"N. anomalies={n_anomalous}\")\n",
    "\n",
    "plt.xlabel(\"False Alarm Rate of detector\", fontsize=13)\n",
    "plt.ylabel(\"F1 score\", fontsize=13)\n",
    "\n",
    "_ = ax.legend(\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.2),\n",
    "    fancybox=True,\n",
    "    shadow=False,\n",
    "    ncol=4,\n",
    "    fontsize=13,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
