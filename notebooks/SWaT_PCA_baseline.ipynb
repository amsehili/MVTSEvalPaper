{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "from util import (\n",
    "    make_intervals,\n",
    "    compute_event_wise_metrics,\n",
    "    predict_with_PCA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load original data files (xlsx) and save them as parquet\n",
    "\n",
    "Loading `xlsx` data with pandas might be very slow. In the following cell, we load original `xlsx` files (as downloaded from iTrust) once and save the as `parquet` for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"./data/SWaT\")\n",
    "\n",
    "df = pd.read_excel(\n",
    "    DATADIR / \"SWaT_Dataset_Normal_v1.xlsx\",\n",
    "    skiprows=[0],\n",
    "    parse_dates=[\" Timestamp\"],\n",
    "    date_format=\" %d/%m/%Y %I:%M:%S %p\",\n",
    "    index_col=\" Timestamp\",\n",
    ")\n",
    "df.index.name = \"Timestamp\"\n",
    "df.to_parquet(DATADIR / \"SWaT_Dataset_Normal_v1.parquet\")\n",
    "\n",
    "DATADIR = Path(\"/home/amine/Workspace/data/SWaT/\")\n",
    "\n",
    "df = pd.read_excel(\n",
    "    DATADIR / \"SWaT_Dataset_Attack_v0.xlsx\",\n",
    "    skiprows=[0],\n",
    "    parse_dates=[\" Timestamp\"],\n",
    "    date_format=\" %d/%m/%Y %I:%M:%S %p\",\n",
    "    index_col=\" Timestamp\",\n",
    ")\n",
    "df.index.name = \"Timestamp\"\n",
    "df.to_parquet(DATADIR / \"clean\" / \"SWaT_Dataset_Attack_v0.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"./data/SWaT\")\n",
    "normal = pd.read_parquet(DATADIR / \"SWaT_Dataset_Normal_v1.parquet\")\n",
    "attack = pd.read_parquet(DATADIR / \"SWaT_Dataset_Attack_v0.parquet\")\n",
    "labels = (attack[\"Normal/Attack\"] == \"Attack\").astype(int)\n",
    "y_true = labels.to_numpy() == 1\n",
    "\n",
    "attacks_ts = pd.read_csv(\n",
    "    DATADIR / \"SWaT_Dataset_v0_attacks_timestamps.csv\",\n",
    "    parse_dates=[\"StartTime\", \"EndTime\"],\n",
    "    date_format=\"%d/%m/%Y %H:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Labels\n",
    "Labels from `attack[\"Normal/Attack\"]` do not reflect the attacks provided by SWaT's owner in `List_of_attacks_Final_2015.pdf`. You can still use `attack[\"Normal/Attack\"]` by skipping the next two cells and passing `gt_intervals=None` when calling `compute_event_wise_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination rate: 11.98\n",
      "Number of anomalous events: 35\n",
      "Min event length: 101\n",
      "Max event length: 34209\n",
      "Average event length: 1540\n",
      "Median event length: 444\n"
     ]
    }
   ],
   "source": [
    "y_true_ts = np.zeros(len(labels))\n",
    "gt_intervals = []\n",
    "index = list(attack.index)\n",
    "for _, (onset, offset) in attacks_ts.iterrows():\n",
    "    onset = index.index(onset)\n",
    "    offset = index.index(offset) + 1\n",
    "    y_true_ts[onset:offset] = 1\n",
    "    gt_intervals.append((onset, offset))\n",
    "y_true_ts.mean()\n",
    "y_true = y_true_ts == 1\n",
    "\n",
    "print(\"Contamination rate:\", f\"{y_true.mean()*100:.2f}\")\n",
    "print(\"Number of anomalous events:\", len(gt_intervals))\n",
    "event_lengths = np.diff(gt_intervals).reshape(-1)\n",
    "print(\"Min event length:\", np.min(event_lengths))\n",
    "print(\"Max event length:\", np.max(event_lengths))\n",
    "print(\"Average event length:\", round(np.mean(event_lengths)))\n",
    "print(\"Median event length:\", round(np.median(event_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "This is the most sensitive part of the whole experiment. Most works use loads of hyperparameters and report the best obtained results, often without disclosing or explaining the choice of hyperparameters' values.\n",
    "\n",
    "In the following, we set the values of hyperparameters related to feature scaling, clipping, number of PCA components, and history size for score smoothing. These values yield state-of-the-art performance with the considered metrics, but you can achieve better performance for some metrics by tweaking some of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "MIN_SCALE = -1.0\n",
    "MAX_SCALE = 1.0\n",
    "\n",
    "MIN_CLIP = -5.0\n",
    "MAX_CLIP = 5.0\n",
    "\n",
    "PCA_N_COMP = 22\n",
    "SCORE_SMOOTHING_HISTORY_SIZE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normal.drop(columns=[\"Normal/Attack\"]).to_numpy().copy()\n",
    "nb_features = X_train.shape[1]\n",
    "X_test = attack.drop(columns=[\"Normal/Attack\"]).to_numpy().copy()\n",
    "\n",
    "scaler = MinMaxScaler((MIN_SCALE, MAX_SCALE))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "pca = PCA(n_components=PCA_N_COMP)\n",
    "pca.fit(X_train)\n",
    "\n",
    "residual = predict_with_PCA(\n",
    "    pca,\n",
    "    X_test.clip(min=MIN_CLIP, max=MAX_CLIP),\n",
    "    smooth_n=SCORE_SMOOTHING_HISTORY_SIZE,\n",
    ")\n",
    "scores = residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Point-wise F1 score\n",
    "We first compute the best point-wise F1 score, then we use the corresponding threshold to compute other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8096"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "\n",
    "f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "round(f1_scores.max(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-wise metrics\n",
    "For these we use threshold that yields the best **point-wise** F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = f1_scores.argmax()\n",
    "best_threshold = thresholds[argmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point-wise F1:\t\t 0.8096\n",
      "Event-wise F1 (F1_ew):\t 0.5551\n",
      "Composite F1 (F1_c):\t 0.5955\n",
      "True positive events:\t 15\n",
      "False positive events:\t 4\n"
     ]
    }
   ],
   "source": [
    "y_pred = scores >= best_threshold\n",
    "\n",
    "\n",
    "TP_ew, FP_ew, FN_ew, P_ew, R_ew, F1_ew, F1_c = compute_event_wise_metrics(\n",
    "    y_true, y_pred, gt_intervals\n",
    ")\n",
    "\n",
    "print(\"Point-wise F1:\\t\\t\", round(f1_score(y_true, y_pred), 4))\n",
    "print(\"Event-wise F1 (F1_ew):\\t\", round(F1_ew, 4))\n",
    "print(\"Composite F1 (F1_c):\\t\", round(F1_c, 4))\n",
    "print(\"True positive events:\\t\", TP_ew)\n",
    "print(\"False positive events:\\t\", FP_ew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics without smoothing\n",
    "\n",
    "You can achieve a much better **composite F1** score by setting `SCORE_SMOOTHING_HISTORY_SIZE` to `None` (no smoothing) instead of 30. This will have little impact on the **point-wise F1**, but the **event-wise** score will decrease considerably due to a high number of false alarms at the event level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point-wise F1:\t\t 0.8015\n",
      "Event-wise F1 (F1_ew):\t 0.0743\n",
      "Composite F1 (F1_c):\t 0.7579\n",
      "True positive events:\t 22\n",
      "False positive events:\t 533\n"
     ]
    }
   ],
   "source": [
    "y_pred = scores >= best_threshold\n",
    "\n",
    "\n",
    "TP_ew, FP_ew, FN_ew, P_ew, R_ew, F1_ew, F1_c = compute_event_wise_metrics(\n",
    "    y_true, y_pred, gt_intervals\n",
    ")\n",
    "\n",
    "print(\"Point-wise F1:\\t\\t\", round(f1_score(y_true, y_pred), 4))\n",
    "print(\"Event-wise F1 (F1_ew):\\t\", round(F1_ew, 4))\n",
    "print(\"Composite F1 (F1_c):\\t\", round(F1_c, 4))\n",
    "print(\"True positive events:\\t\", TP_ew)\n",
    "print(\"False positive events:\\t\", FP_ew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
